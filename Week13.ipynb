{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week13.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPIcXeLCzSY90vVHfgplmTM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adzanlr/Machine-Learning-Week-13/blob/main/Week13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjFOxbaxFoKg",
        "outputId": "0bf4918a-75d4-415a-f1ac-4b164e69da50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "from numpy import newaxis\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.layers.recurrent import LSTM, GRU\n",
        "from keras.models import Sequential\n",
        "from keras import optimizers\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "print ('import completed')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3CpapumFqch",
        "outputId": "8ae634bb-e511-4791-fddc-48c4c6e7cdf5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter in how much steps we will enroll the network.\n",
        "# RNN/LSTM/GRU can be taught patterns over times series as big as the number of times you enrol them, and no bigger (fundamental limitation). \n",
        "# So by design these networks are deep/long to catch recurrent patterns.\n",
        "Enrol_window = 100\n",
        "\n",
        "print ('enrol window set to',Enrol_window )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BD5gyTxuFv0d",
        "outputId": "1611ac8e-fe9a-4637-d15d-3192868252c1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enrol window set to 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Support functions\n",
        "sc = MinMaxScaler(feature_range=(0,1))\n",
        "def load_data(datasetname, column, seq_len, normalise_window):\n",
        "    # A support function to help prepare datasets for an RNN/LSTM/GRU\n",
        "    data = datasetname.loc[:,column]\n",
        "\n",
        "    sequence_length = seq_len + 1\n",
        "    result = []\n",
        "    for index in range(len(data) - sequence_length):\n",
        "        result.append(data[index: index + sequence_length])\n",
        "    \n",
        "    if normalise_window:\n",
        "        #result = sc.fit_transform(result)\n",
        "        result = normalise_windows(result)\n",
        "\n",
        "    result = np.array(result)\n",
        "\n",
        "    #Last 10% is used for validation test, first 90% for training\n",
        "    row = round(0.9 * result.shape[0])\n",
        "    train = result[:int(row), :]\n",
        "    np.random.shuffle(train)\n",
        "    x_train = train[:, :-1]\n",
        "    y_train = train[:, -1]\n",
        "    x_test = result[int(row):, :-1]\n",
        "    y_test = result[int(row):, -1]\n",
        "\n",
        "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
        "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))  \n",
        "\n",
        "    return [x_train, y_train, x_test, y_test]\n",
        "\n",
        "def normalise_windows(window_data):\n",
        "    # A support function to normalize a dataset\n",
        "    normalised_data = []\n",
        "    for window in window_data:\n",
        "        normalised_window = [((float(p) / float(window[0])) - 1) for p in window]\n",
        "        normalised_data.append(normalised_window)\n",
        "    return normalised_data\n",
        "\n",
        "def predict_sequence_full(model, data, window_size):\n",
        "    #Shift the window by 1 new prediction each time, re-run predictions on new window\n",
        "    curr_frame = data[0]\n",
        "    predicted = []\n",
        "    for i in range(len(data)):\n",
        "        predicted.append(model.predict(curr_frame[newaxis,:,:])[0,0])\n",
        "        curr_frame = curr_frame[1:]\n",
        "        curr_frame = np.insert(curr_frame, [window_size-1], predicted[-1], axis=0)\n",
        "    return predicted\n",
        "\n",
        "def predict_sequences_multiple(model, data, window_size, prediction_len):\n",
        "    #Predict sequence of <prediction_len> steps before shifting prediction run forward by <prediction_len> steps\n",
        "    prediction_seqs = []\n",
        "    for i in range(int(len(data)/prediction_len)):\n",
        "        curr_frame = data[i*prediction_len]\n",
        "        predicted = []\n",
        "        for j in range(prediction_len):\n",
        "            predicted.append(model.predict(curr_frame[newaxis,:,:])[0,0])\n",
        "            curr_frame = curr_frame[1:]\n",
        "            curr_frame = np.insert(curr_frame, [window_size-1], predicted[-1], axis=0)\n",
        "        prediction_seqs.append(predicted)\n",
        "    return prediction_seqs\n",
        "\n",
        "def plot_results(predicted_data, true_data): \n",
        "    fig = plt.figure(facecolor='white') \n",
        "    ax = fig.add_subplot(111) \n",
        "    ax.plot(true_data, label='True Data') \n",
        "    plt.plot(predicted_data, label='Prediction') \n",
        "    plt.legend() \n",
        "    plt.show() \n",
        "    \n",
        "def plot_results_multiple(predicted_data, true_data, prediction_len):\n",
        "    fig = plt.figure(facecolor='white')\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.plot(true_data, label='True Data')\n",
        "    #Pad the list of predictions to shift it in the graph to it's correct start\n",
        "    for i, data in enumerate(predicted_data):\n",
        "        padding = [None for p in range(i * prediction_len)]\n",
        "        plt.plot(padding + data, label='Prediction')\n",
        "        plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "print ('Support functions defined')# Support functions\n",
        "sc = MinMaxScaler(feature_range=(0,1))\n",
        "def load_data(datasetname, column, seq_len, normalise_window):\n",
        "    # A support function to help prepare datasets for an RNN/LSTM/GRU\n",
        "    data = datasetname.loc[:,column]\n",
        "\n",
        "    sequence_length = seq_len + 1\n",
        "    result = []\n",
        "    for index in range(len(data) - sequence_length):\n",
        "        result.append(data[index: index + sequence_length])\n",
        "    \n",
        "    if normalise_window:\n",
        "        #result = sc.fit_transform(result)\n",
        "        result = normalise_windows(result)\n",
        "\n",
        "    result = np.array(result)\n",
        "\n",
        "    #Last 10% is used for validation test, first 90% for training\n",
        "    row = round(0.9 * result.shape[0])\n",
        "    train = result[:int(row), :]\n",
        "    np.random.shuffle(train)\n",
        "    x_train = train[:, :-1]\n",
        "    y_train = train[:, -1]\n",
        "    x_test = result[int(row):, :-1]\n",
        "    y_test = result[int(row):, -1]\n",
        "\n",
        "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
        "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))  \n",
        "\n",
        "    return [x_train, y_train, x_test, y_test]\n",
        "\n",
        "def normalise_windows(window_data):\n",
        "    # A support function to normalize a dataset\n",
        "    normalised_data = []\n",
        "    for window in window_data:\n",
        "        normalised_window = [((float(p) / float(window[0])) - 1) for p in window]\n",
        "        normalised_data.append(normalised_window)\n",
        "    return normalised_data\n",
        "\n",
        "def predict_sequence_full(model, data, window_size):\n",
        "    #Shift the window by 1 new prediction each time, re-run predictions on new window\n",
        "    curr_frame = data[0]\n",
        "    predicted = []\n",
        "    for i in range(len(data)):\n",
        "        predicted.append(model.predict(curr_frame[newaxis,:,:])[0,0])\n",
        "        curr_frame = curr_frame[1:]\n",
        "        curr_frame = np.insert(curr_frame, [window_size-1], predicted[-1], axis=0)\n",
        "    return predicted\n",
        "\n",
        "def predict_sequences_multiple(model, data, window_size, prediction_len):\n",
        "    #Predict sequence of <prediction_len> steps before shifting prediction run forward by <prediction_len> steps\n",
        "    prediction_seqs = []\n",
        "    for i in range(int(len(data)/prediction_len)):\n",
        "        curr_frame = data[i*prediction_len]\n",
        "        predicted = []\n",
        "        for j in range(prediction_len):\n",
        "            predicted.append(model.predict(curr_frame[newaxis,:,:])[0,0])\n",
        "            curr_frame = curr_frame[1:]\n",
        "            curr_frame = np.insert(curr_frame, [window_size-1], predicted[-1], axis=0)\n",
        "        prediction_seqs.append(predicted)\n",
        "    return prediction_seqs\n",
        "\n",
        "def plot_results(predicted_data, true_data): \n",
        "    fig = plt.figure(facecolor='white') \n",
        "    ax = fig.add_subplot(111) \n",
        "    ax.plot(true_data, label='True Data') \n",
        "    plt.plot(predicted_data, label='Prediction') \n",
        "    plt.legend() \n",
        "    plt.show() \n",
        "    \n",
        "def plot_results_multiple(predicted_data, true_data, prediction_len):\n",
        "    fig = plt.figure(facecolor='white')\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.plot(true_data, label='True Data')\n",
        "    #Pad the list of predictions to shift it in the graph to it's correct start\n",
        "    for i, data in enumerate(predicted_data):\n",
        "        padding = [None for p in range(i * prediction_len)]\n",
        "        plt.plot(padding + data, label='Prediction')\n",
        "        plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "print ('Support functions defined')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26wmiFzXFzYB",
        "outputId": "db6ae6ff-5d64-47ee-d165-af361bb14a27"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Support functions defined\n",
            "Support functions defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "dataset = pd.read_csv('drive/MyDrive/dataset/Sin Wave Data Generator.csv')\n",
        "dataset[\"Wave\"][:].plot(figsize=(16,4),legend=False)"
      ],
      "metadata": {
        "id": "-DGuMWndGe-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the dataset, note that all data foer the sinus wave is already normalized between 0 and 1\n",
        "# A label is the thing we're predicting\n",
        "# A feature is an input variable, in this case a stock price\n",
        "\n",
        "feature_train, label_train, feature_test, label_test = load_data(dataset, 'Wave', Enrol_window, False)\n",
        "\n",
        "print ('Datasets generated')"
      ],
      "metadata": {
        "id": "WbtUOQxGF41R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The LSTM model I would like to test\n",
        "# Note: replace LSTM with GRU or RNN if you want to try those\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, return_sequences=True, input_shape=(feature_train.shape[1],1)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100, return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation = \"linear\"))\n",
        "\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "print ('model compiled')\n",
        "\n",
        "print (model.summary())"
      ],
      "metadata": {
        "id": "oQDbQeH3GhYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "model.fit(feature_train, label_train, batch_size=512, epochs=10, validation_data = (feature_test, label_test))"
      ],
      "metadata": {
        "id": "8bXAKJViGjpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch 1/10\n",
        "9/9 [==============================] - 21s 2s/step - loss: 0.2281 - val_loss: 0.1045\n",
        "Epoch 2/10\n",
        "9/9 [==============================] - 14s 1s/step - loss: 0.0599 - val_loss: 0.0229\n",
        "Epoch 3/10\n",
        "9/9 [==============================] - 13s 1s/step - loss: 0.0123 - val_loss: 0.0077\n",
        "Epoch 4/10\n",
        "9/9 [==============================] - 13s 1s/step - loss: 0.0089 - val_loss: 0.0014\n",
        "Epoch 5/10\n",
        "9/9 [==============================] - 13s 1s/step - loss: 0.0063 - val_loss: 0.0018\n",
        "Epoch 6/10\n",
        "9/9 [==============================] - 13s 1s/step - loss: 0.0048 - val_loss: 2.3805e-04\n",
        "Epoch 7/10\n",
        "9/9 [==============================] - 13s 1s/step - loss: 0.0043 - val_loss: 3.2758e-04\n",
        "Epoch 8/10\n",
        "9/9 [==============================] - 13s 1s/step - loss: 0.0040 - val_loss: 3.4824e-04\n",
        "Epoch 9/10\n",
        "9/9 [==============================] - 13s 1s/step - loss: 0.0038 - val_loss: 1.5387e-04\n",
        "Epoch 10/10\n",
        "9/9 [==============================] - 13s 1s/step - loss: 0.0037 - val_loss: 1.1707e-04\n",
        "<keras.callbacks.History at 0x7f27835d1d50>"
      ],
      "metadata": {
        "id": "Pi5lSnr1Glpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's use the model and predict the wave\n",
        "predictions= predict_sequence_full(model, feature_test, Enrol_window)\n",
        "plot_results(predictions,label_test)"
      ],
      "metadata": {
        "id": "0EZmJ7OCGmwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's get the stock data\n",
        "dataset = pd.read_csv('drive/MyDrive/dataset/IBM_2006-01-01_to_2018-01-01.csv', index_col='Date', parse_dates=['Date'])\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "M8oKJRUgGohX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the dataset, note that the stock price data will be normalized between 0 and 1\n",
        "# A label is the thing we're predicting\n",
        "# A feature is an input variable, in this case a stock price\n",
        "# Selected 'Close' (stock pric at closing) attribute for prices. Let's see what it looks like\n",
        "\n",
        "feature_train, label_train, feature_test, label_test = load_data(dataset, 'Close', Enrol_window, True)\n",
        "\n",
        "dataset[\"Close\"][:'2016'].plot(figsize=(16,4),legend=True)\n",
        "dataset[\"Close\"]['2017':].plot(figsize=(16,4),legend=True) # 10% is used for thraining data which is approx 2017 data\n",
        "plt.legend(['Training set (First 90%, approx before 2017)','Test set (Last 10%, approax 2017 and beyond)'])\n",
        "plt.title('IBM stock price')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NOMNJdd4HNhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The same LSTM model I would like to test, lets see if the sinus prediction results can be matched\n",
        "# Note: replace LSTM with GRU or RNN if you want to try those\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, return_sequences=True, input_shape=(feature_train.shape[1],1)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100, return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation = \"linear\"))\n",
        "\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "print ('model compiled')\n",
        "\n",
        "print (model.summary())"
      ],
      "metadata": {
        "id": "65P51Rz5HOrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "model.fit(feature_train, label_train, batch_size=512, epochs=5, validation_data = (feature_test, label_test))"
      ],
      "metadata": {
        "id": "vOfJIX9rHQAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch 1/5\n",
        "6/6 [==============================] - 14s 1s/step - loss: 0.0112 - val_loss: 0.0036\n",
        "Epoch 2/5\n",
        "6/6 [==============================] - 8s 1s/step - loss: 0.0025 - val_loss: 0.0019\n",
        "Epoch 3/5\n",
        "6/6 [==============================] - 8s 1s/step - loss: 0.0014 - val_loss: 0.0012\n",
        "Epoch 4/5\n",
        "6/6 [==============================] - 8s 1s/step - loss: 0.0014 - val_loss: 6.0192e-04\n",
        "Epoch 5/5\n",
        "6/6 [==============================] - 8s 1s/step - loss: 9.9286e-04 - val_loss: 6.8863e-04\n",
        "<keras.callbacks.History at 0x7f277e7fa110>"
      ],
      "metadata": {
        "id": "nm4AapyIHSJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's use the model and predict the stock\n",
        "predicted_stock_price = model.predict(feature_test)\n",
        "plot_results(predicted_stock_price,label_test)"
      ],
      "metadata": {
        "id": "SM3_DqYiHQEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = predict_sequences_multiple(model, feature_test, Enrol_window, 50)\n",
        "plot_results_multiple(predictions, label_test, 50)"
      ],
      "metadata": {
        "id": "yV26TmNzHUcY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}